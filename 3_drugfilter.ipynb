{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "import time\n",
    "import re\n",
    "import glob\n",
    "import tqdm\n",
    "### Extract sentiment and NER recognition (Drug, Symptom) and split them into three columns\n",
    "# Create list of filenames\n",
    "import glob\n",
    "import pandas as pd\n",
    "import tqdm   ## this is a ticker\n",
    "data_dir = \"/data1/covid_sentiment_decode/\" \n",
    "files = sorted(glob.glob(data_dir+\"*.csv\"))\n",
    "file = '|'.join(files) # Convert to string\n",
    "\n",
    "# Process files in loop\n",
    "import pandas as pd \n",
    "out_dir = '/data1/data_8t/ivermectin/sentiment/' # Location to save files\n",
    "data_type = {'created_at':'string','id':'string','full_text':'string','user':'object',\"extracted_entity_sentiment\":'object',\"extracted_entity\":'object'}\n",
    "\n",
    "for file in tqdm.tqdm(files):\n",
    "    df = pd.read_csv(file, dtype=data_type, usecols=['created_at','id','full_text','user','extracted_entity_sentiment',\"extracted_entity\"],lineterminator='\\n',keep_default_na=False)\n",
    "    raw_len = len(df) # Total number of tweets in file\n",
    "    print(raw_len)\n",
    "    keys='Drug'\n",
    "\n",
    "    df = df[df[\"extracted_entity_sentiment\"].str.contains(fr\"\\b(?:{keys})\\b\",case=False)] # Filter data containing keys\n",
    "    df = df.drop_duplicates(subset=['id']) # Remove duplicates\n",
    "    print(len(df))\n",
    "    print('-----------------------------------------')\n",
    "\n",
    "    df.to_csv(out_dir+file.split(\"/\")[-1],index=False)  # Save file\n",
    "def Drug_Entity(entity): # Extract drug entities from entity extraction section\n",
    "    drug_entity = []\n",
    "    try:\n",
    "        for i in eval(entity): \n",
    "            if i['type'] == 'Drug':\n",
    "                drug_entity.append(i['value'].lower())\n",
    "        return drug_entity\n",
    "    except TypeError: # because some users has no text\n",
    "        return '[]'\n",
    "    \n",
    "def Symp_Entity(entity): # Extract symptom entities from entity extraction section\n",
    "    symp_entity = []\n",
    "    try:\n",
    "        for i in eval(entity): \n",
    "            if i['type'] == 'Symptom':\n",
    "                symp_entity.append(i['value'].lower())\n",
    "        return symp_entity\n",
    "    except TypeError: # because some users has no text\n",
    "        return '[]'\n",
    "\n",
    "def Senti_Entity(entity): # Extract drug entities from entity extraction section\n",
    "    senti_entity = []\n",
    "    try:\n",
    "        for i in eval(entity): \n",
    "            if i['type'] == 'Drug' and 'sentiment' in i:\n",
    "                senti_entity.append(i['sentiment'].lower())\n",
    "        return senti_entity\n",
    "    except TypeError: # because some users has no text\n",
    "        return '[]'\n",
    "\n",
    "def parall_func(df):\n",
    "    #df['keywords'] = df['full_text'].apply(getDrug)\n",
    "    df['drug_entity'] = df['extracted_entity'].apply(Drug_Entity)\n",
    "    df['sentiment']=df['extracted_entity_sentiment'].apply(Senti_Entity)\n",
    "    return df\n",
    "\n",
    "\n",
    "#pattern = re.compile('|'.join(symptom_keywords),flags=re.I)\n",
    "# This function is used for multithreading, input is the dataframe you want to process and the corresponding function\n",
    "def parallelize_dataframe(df, func, **kwargs):\n",
    "    #CPUs = multiprocessing.cpu_count()\n",
    "    num_partitions = 4 # number of partitions to split dataframe\n",
    "    num_cores =  4 # number of cores on your machine\n",
    "\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    func = partial(func, **kwargs)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "'''\n",
    "# find the keywords in the text. if a keyword appears \n",
    "# several time in a text, only return one of it.\n",
    "'''\n",
    "import glob\n",
    "import pandas as pd\n",
    "import tqdm   ## this is a ticker\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import tqdm \n",
    "data_dir = \"/data1/data_8t/ivermectin/sentiment/\" \n",
    "out_dir='/data1/data_8t/ivermectin/sentiment/drug+senti/' # Folder to place results\n",
    "files = sorted(glob.glob(data_dir+\"*.csv\"))\n",
    "for file in tqdm.tqdm(files):\n",
    "    df = pd.read_csv(file,lineterminator='\\n',low_memory=False)\n",
    "    df = parallelize_dataframe(df, parall_func)\n",
    "    df.to_csv(out_dir+file.split(\"/\")[-1],index=False)  # Save file"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
