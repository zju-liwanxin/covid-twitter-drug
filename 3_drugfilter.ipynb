{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "import time\n",
    "import re\n",
    "import glob\n",
    "import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将sentiment和ner识别(Drug,Symptom)都提取出来，分成三列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建文件名列表\n",
    "import glob\n",
    "import pandas as pd\n",
    "import tqdm   ## this is a ticker\n",
    "data_dir = \"/data1/covid_sentiment_decode/\" \n",
    "files = sorted(glob.glob(data_dir+\"*.csv\"))\n",
    "file = '|'.join(files) #转为字符串\n",
    "\n",
    "#循环处理文件\n",
    "import pandas as pd \n",
    "out_dir = '/data1/data_8t/ivermectin/sentiment/' #保存文件的位置\n",
    "data_type = {'created_at':'string','id':'string','full_text':'string','user':'object',\"extracted_entity_sentiment\":'object',\"extracted_entity\":'object'}\n",
    "\n",
    "for file in tqdm.tqdm(files):\n",
    "    df = pd.read_csv(file, dtype=data_type, usecols=['created_at','id','full_text','user','extracted_entity_sentiment',\"extracted_entity\"],lineterminator='\\n',keep_default_na=False)\n",
    "    raw_len = len(df) # 文件tweets总数\n",
    "    print(raw_len)\n",
    "    keys='Drug'\n",
    "\n",
    "    df = df[df[\"extracted_entity_sentiment\"].str.contains(fr\"\\b(?:{keys})\\b\",case=False)] #筛选包含keys的数据\n",
    "    df = df.drop_duplicates(subset=['id']) #去除重复项\n",
    "    print(len(df))\n",
    "    print('-----------------------------------------')\n",
    "\n",
    "    df.to_csv(out_dir+file.split(\"/\")[-1],index=False)  #保存文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Drug_Entity(entity):#实体提取部分提取出drug实体\n",
    "    drug_entity = []\n",
    "    try:\n",
    "        for i in eval(entity): \n",
    "            if i['type'] == 'Drug':\n",
    "                drug_entity.append(i['value'].lower())\n",
    "        return drug_entity\n",
    "    except TypeError: # because some users has no text\n",
    "        return '[]'\n",
    "    \n",
    "def Symp_Entity(entity):#实体提取部分提取出symptom实体\n",
    "    symp_entity = []\n",
    "    try:\n",
    "        for i in eval(entity): \n",
    "            if i['type'] == 'Symptom':\n",
    "                symp_entity.append(i['value'].lower())\n",
    "        return symp_entity\n",
    "    except TypeError: # because some users has no text\n",
    "        return '[]'\n",
    "\n",
    "def Senti_Entity(entity):#实体提取部分提取出drug实体\n",
    "    senti_entity = []\n",
    "    try:\n",
    "        for i in eval(entity): \n",
    "            if i['type'] == 'Drug' and 'sentiment' in i:\n",
    "                senti_entity.append(i['sentiment'].lower())\n",
    "        return senti_entity\n",
    "    except TypeError: # because some users has no text\n",
    "        return '[]'\n",
    "\n",
    "def parall_func(df):\n",
    "    #df['keywords'] = df['full_text'].apply(getDrug)\n",
    "    df['drug_entity'] = df['extracted_entity'].apply(Drug_Entity)\n",
    "    df['sentiment']=df['extracted_entity_sentiment'].apply(Senti_Entity)\n",
    "    return df\n",
    "\n",
    "\n",
    "#pattern = re.compile('|'.join(symptom_keywords),flags=re.I)\n",
    "# 这个函数就是用来做多线程的，输入是你要处理的数据框和相应的函数\n",
    "def parallelize_dataframe(df, func, **kwargs):\n",
    "    #CPUs = multiprocessing.cpu_count()\n",
    "    num_partitions = 4 # number of partitions to split dataframe\n",
    "    num_cores =  4 # number of cores on your machine\n",
    "\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    func = partial(func, **kwargs)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "'''\n",
    "# find the keywords in the text. if a keyword appears \n",
    "# several time in a text, only return one of it.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import tqdm   ## this is a ticker\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import tqdm \n",
    "data_dir = \"/data1/data_8t/ivermectin/sentiment/\" # 目前这个文件夹下已没有135个文件\n",
    "out_dir='/data1/data_8t/ivermectin/sentiment/drug+senti/'#放置结果的文件夹\n",
    "files = sorted(glob.glob(data_dir+\"*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in tqdm.tqdm(files):\n",
    "    df = pd.read_csv(file,lineterminator='\\n',low_memory=False)\n",
    "    df = parallelize_dataframe(df, parall_func)\n",
    "    df.to_csv(out_dir+file.split(\"/\")[-1],index=False)  #保存文件"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('lwx')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "765ef3d65742a26d9e89c0bc1eecd9f137f8dcbf3a9b284d9226ff30499408a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
